{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# .envファイルからAPIキーを読み込む\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader \n",
    "\n",
    "# PDFの読み込み\n",
    "file = '../data/GPT-4_Technical_Report.pdf'\n",
    "loader = PyPDFLoader(file)\n",
    "\n",
    "# PDFをページごとに分割\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='GPT-4 Technical Report\\nOpenAI\\x03\\nAbstract\\nWe report the development of GPT-4, a large-scale, multimodal model which can\\naccept image and text inputs and produce text outputs. While less capable than\\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance\\non various professional and academic benchmarks, including passing a simulated\\nbar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-\\nbased model pre-trained to predict the next token in a document. The post-training\\nalignment process results in improved performance on measures of factuality and\\nadherence to desired behavior. A core component of this project was developing\\ninfrastructure and optimization methods that behave predictably across a wide\\nrange of scales. This allowed us to accurately predict some aspects of GPT-4’s\\nperformance based on models trained with no more than 1/1,000th the compute of\\nGPT-4.\\n1 Introduction\\nThis technical report presents GPT-4, a large multimodal model capable of processing image and\\ntext inputs and producing text outputs. Such models are an important area of study as they have the\\npotential to be used in a wide range of applications, such as dialogue systems, text summarization,\\nand machine translation. As such, they have been the subject of substantial interest and progress in\\nrecent years [1–34].\\nOne of the main goals of developing such models is to improve their ability to understand and generate\\nnatural language text, particularly in more complex and nuanced scenarios. To test its capabilities\\nin such scenarios, GPT-4 was evaluated on a variety of exams originally designed for humans. In\\nthese evaluations it performs quite well and often outscores the vast majority of human test takers.\\nFor example, on a simulated bar exam, GPT-4 achieves a score that falls in the top 10% of test takers.\\nThis contrasts with GPT-3.5, which scores in the bottom 10%.\\nOn a suite of traditional NLP benchmarks, GPT-4 outperforms both previous large language models\\nand most state-of-the-art systems (which often have benchmark-speciﬁc training or hand-engineering).\\nOn the MMLU benchmark [ 35,36], an English-language suite of multiple-choice questions covering\\n57 subjects, GPT-4 not only outperforms existing models by a considerable margin in English, but\\nalso demonstrates strong performance in other languages. On translated variants of MMLU, GPT-4\\nsurpasses the English-language state-of-the-art in 24 of 26 languages considered. We discuss these\\nmodel capability results, as well as model safety improvements and results, in more detail in later\\nsections.\\nThis report also discusses a key challenge of the project, developing deep learning infrastructure and\\noptimization methods that behave predictably across a wide range of scales. This allowed us to make\\npredictions about the expected performance of GPT-4 (based on small runs trained in similar ways)\\nthat were tested against the ﬁnal run to increase conﬁdence in our training.\\nDespite its capabilities, GPT-4 has similar limitations to earlier GPT models [ 1,37,38]: it is not fully\\nreliable (e.g. can suffer from “hallucinations”), has a limited context window, and does not learn\\n\\x03Please cite this work as “OpenAI (2023)\". Full authorship contribution statements appear at the end of the\\ndocument. Correspondence regarding this technical report can be sent to gpt4-report@openai.comarXiv:2303.08774v3  [cs.CL]  27 Mar 2023', metadata={'source': '../data/GPT-4_Technical_Report.pdf', 'page': 0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベクターストアの作成(既存のドキュメントを構造化して保存するためのデータベース)\n",
    "from langchain.vectorstores import DocArrayInMemorySearch \n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# ベクターストアの作成\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(\n",
    "    documents=pages,\n",
    "    embedding=OpenAIEmbeddings(disallowed_special=()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Encountered text corresponding to disallowed special token '<|endofprompt|>'.\nIf you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endofprompt|>', ...}`.\nIf you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endofprompt|>'})`.\nTo disable this check for all special tokens, pass `disallowed_special=()`.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# ベクターストアの作成\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m db \u001b[39m=\u001b[39m DocArrayInMemorySearch\u001b[39m.\u001b[39;49mfrom_documents(\n\u001b[1;32m      3\u001b[0m     documents\u001b[39m=\u001b[39;49mpages,\n\u001b[1;32m      4\u001b[0m     embedding\u001b[39m=\u001b[39;49mOpenAIEmbeddings(),\n\u001b[1;32m      5\u001b[0m )\n",
      "File \u001b[0;32m~/projects/github.com/koonn/langchain_prediction/.venv/lib/python3.10/site-packages/langchain/vectorstores/base.py:332\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m texts \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m    331\u001b[0m metadatas \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m--> 332\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_texts(texts, embedding, metadatas\u001b[39m=\u001b[39;49mmetadatas, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/projects/github.com/koonn/langchain_prediction/.venv/lib/python3.10/site-packages/langchain/vectorstores/docarray/in_memory.py:68\u001b[0m, in \u001b[0;36mDocArrayInMemorySearch.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create an DocArrayInMemorySearch store and insert data.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m    DocArrayInMemorySearch Vector Store\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m store \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfrom_params(embedding, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 68\u001b[0m store\u001b[39m.\u001b[39;49madd_texts(texts\u001b[39m=\u001b[39;49mtexts, metadatas\u001b[39m=\u001b[39;49mmetadatas)\n\u001b[1;32m     69\u001b[0m \u001b[39mreturn\u001b[39;00m store\n",
      "File \u001b[0;32m~/projects/github.com/koonn/langchain_prediction/.venv/lib/python3.10/site-packages/langchain/vectorstores/docarray/base.py:80\u001b[0m, in \u001b[0;36mDocArrayIndex.add_texts\u001b[0;34m(self, texts, metadatas, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Run more texts through the embeddings and add to the vectorstore.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39m    List of ids from adding the texts into the vectorstore.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m ids: List[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 80\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding\u001b[39m.\u001b[39;49membed_documents(\u001b[39mlist\u001b[39;49m(texts))\n\u001b[1;32m     81\u001b[0m \u001b[39mfor\u001b[39;00m i, (t, e) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(texts, embeddings)):\n\u001b[1;32m     82\u001b[0m     m \u001b[39m=\u001b[39m metadatas[i] \u001b[39mif\u001b[39;00m metadatas \u001b[39melse\u001b[39;00m {}\n",
      "File \u001b[0;32m~/projects/github.com/koonn/langchain_prediction/.venv/lib/python3.10/site-packages/langchain/embeddings/openai.py:452\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call out to OpenAI's embedding endpoint for embedding search docs.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \n\u001b[1;32m    442\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[39m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[0;32m--> 452\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_len_safe_embeddings(texts, engine\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeployment)\n",
      "File \u001b[0;32m~/projects/github.com/koonn/langchain_prediction/.venv/lib/python3.10/site-packages/langchain/embeddings/openai.py:290\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m001\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    287\u001b[0m     \u001b[39m# See: https://github.com/openai/openai-python/issues/418#issuecomment-1525939500\u001b[39;00m\n\u001b[1;32m    288\u001b[0m     \u001b[39m# replace newlines, which can negatively affect performance.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 290\u001b[0m token \u001b[39m=\u001b[39m encoding\u001b[39m.\u001b[39;49mencode(\n\u001b[1;32m    291\u001b[0m     text,\n\u001b[1;32m    292\u001b[0m     allowed_special\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mallowed_special,\n\u001b[1;32m    293\u001b[0m     disallowed_special\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdisallowed_special,\n\u001b[1;32m    294\u001b[0m )\n\u001b[1;32m    295\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(token), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_ctx_length):\n\u001b[1;32m    296\u001b[0m     tokens \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [token[j : j \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_ctx_length]]\n",
      "File \u001b[0;32m~/projects/github.com/koonn/langchain_prediction/.venv/lib/python3.10/site-packages/tiktoken/core.py:117\u001b[0m, in \u001b[0;36mEncoding.encode\u001b[0;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[1;32m    115\u001b[0m         disallowed_special \u001b[39m=\u001b[39m \u001b[39mfrozenset\u001b[39m(disallowed_special)\n\u001b[1;32m    116\u001b[0m     \u001b[39mif\u001b[39;00m match \u001b[39m:=\u001b[39m _special_token_regex(disallowed_special)\u001b[39m.\u001b[39msearch(text):\n\u001b[0;32m--> 117\u001b[0m         raise_disallowed_special_token(match\u001b[39m.\u001b[39;49mgroup())\n\u001b[1;32m    119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_core_bpe\u001b[39m.\u001b[39mencode(text, allowed_special)\n",
      "File \u001b[0;32m~/projects/github.com/koonn/langchain_prediction/.venv/lib/python3.10/site-packages/tiktoken/core.py:351\u001b[0m, in \u001b[0;36mraise_disallowed_special_token\u001b[0;34m(token)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_disallowed_special_token\u001b[39m(token: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NoReturn:\n\u001b[0;32m--> 351\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    352\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEncountered text corresponding to disallowed special token \u001b[39m\u001b[39m{\u001b[39;00mtoken\u001b[39m!r}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    353\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf you want this text to be encoded as a special token, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    354\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpass it to `allowed_special`, e.g. `allowed_special=\u001b[39m\u001b[39m{{\u001b[39;00m\u001b[39m{\u001b[39;00mtoken\u001b[39m!r}\u001b[39;00m\u001b[39m, ...\u001b[39m\u001b[39m}}\u001b[39;00m\u001b[39m`.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    355\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIf you want this text to be encoded as normal text, disable the check for this token \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    356\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mby passing `disallowed_special=(enc.special_tokens_set - \u001b[39m\u001b[39m{{\u001b[39;00m\u001b[39m{\u001b[39;00mtoken\u001b[39m!r}\u001b[39;00m\u001b[39m}}\u001b[39;00m\u001b[39m)`.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    357\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo disable this check for all special tokens, pass `disallowed_special=()`.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Encountered text corresponding to disallowed special token '<|endofprompt|>'.\nIf you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endofprompt|>', ...}`.\nIf you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endofprompt|>'})`.\nTo disable this check for all special tokens, pass `disallowed_special=()`.\n"
     ]
    }
   ],
   "source": [
    "# ベクターストアの作成\n",
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    documents=pages,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llmの作成\n",
    "llm = ChatOpenAI(temperature = 0.0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Retrieverの作成\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Q&Aのためのチェインの作成\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = qa_stuff.run('この論文を要約して')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4のInstructGPT論文は、人間のフィードバックを用いて大規模な言語モデルを訓練し、指示に従う能力を向上させることに焦点を当てています。著者は、言語モデルを大きくすること自体が、ユーザーの意図に従う能力を向上させるわけではないと指摘しています。大きなモデルは、真実でない、有害な、または単に役に立たない出力を生成することがあります。\n",
      "\n",
      "この問題に対処するため、著者らは人間のフィードバックを用いて言語モデルを幅広いタスクに対して微調整します。まず、ラベラーが作成したプロンプトと応答のセットから始め、モデルの望ましい振る舞いを示すラベラーのデモンストレーションのデータセットを収集します。彼らは教師あり学習を用いてGPT-3を微調整し、その後、人間のフィードバックに基づいて強化学習を行いモデルをさらに微調整します。その結果、InstructGPTと呼ばれるモデルは、真実性の向上と有害な出力の削減を示し、一般的なNLPデータセットでの性能の低下は最小限です。\n",
      "\n",
      "著者は、人間のフィードバックを用いた微調整が言語モデルを人間の意図とより一致させるための有望な手法であると結論付けています。\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response2 = qa_stuff.run('GPT-3とGPT-4の違いについて、論文を読んで説明してください。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4は、GPT-3と比較していくつかの重要な改善が行われています。まず、GPT-4は画像とテキストの入力を受け付け、テキストの出力を生成することができるマルチモーダルモデルです。これにより、GPT-4はより複雑で微妙なシナリオでの自然言語テキストの理解と生成能力が向上しています。\n",
      "\n",
      "GPT-4は、人間のテストを含むさまざまなプロフェッショナルおよび学術的なベンチマークで人間レベルのパフォーマンスを示しています。たとえば、シミュレートされた司法試験では、GPT-4はテスト受験者の上位10％に入るスコアを達成しています。これに対して、GPT-3.5は下位10％のスコアを示しています。\n",
      "\n",
      "さらに、GPT-4は従来のNLPベンチマークでも優れたパフォーマンスを発揮しています。MMLUベンチマークでは、GPT-4は既存のモデルを大幅に上回るだけでなく、他の言語でも強力なパフォーマンスを示しています。翻訳されたバリアントでは、GPT-4は26の言語のうち24言語で英語の最先端を超えています。\n",
      "\n",
      "ただし、GPT-4にはいくつかの制約もあります。まず、完全に信頼性があるわけではなく、事実を「幻覚」させたり、推論エラーを起こすことがあります。また、GPT-4は限られたコンテキストウィンドウを持ち、経験から学習することはありません。さらに、GPT-4はセキュリティの脆弱性を導入するなど、難しい問題に対しても人間と同様に失敗することがあります。\n",
      "\n",
      "以上が、GPT-3とGPT-4の主な違いです。詳細な情報は、論文を参照してください。\n"
     ]
    }
   ],
   "source": [
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response3 = qa_stuff.run('事実を幻覚させるとはどういうことですか？論文を読んで教えてください。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「事実を幻覚させる」とは、GPT-4のような言語モデルが、特定の情報源に関連していない、非現実的または真実ではない内容を生成することを指します。これは、モデルがますます説得力があり信じられるようになるにつれて、ユーザーがそれに過度に依存することを引き起こす可能性があります。この傾向は特に有害であり、モデルがユーザーに真実の情報を提供することで信頼を築いた場合により危険となります。また、これらのモデルが社会に統合され、さまざまなシステムの自動化に役立つようになるにつれて、幻覚の傾向は情報の総合的な品質の低下や、自由に利用できる情報の信頼性の低下につながる要因の一つとなります。\n",
      "\n",
      "GPT-4の幻覚のポテンシャルを評価するために、閉じたドメインとオープンドメインの両方のコンテキストで測定を行いました。閉じたドメインの幻覚は、自動評価（GPT-4をゼロショットクラシファイアとして使用）と人間の評価を使用して測定しました。オープンドメインの幻覚については、事実ではないとフラグが付けられた実世界のデータを収集し、それをレビューして「事実」となるセットを作成しました。これを使用してモデルの生成物を「事実」のセットと比較し、人間の評価を容易にしました。\n",
      "\n",
      "GPT-4は、ChatGPTなどの以前のモデルからのデータを活用して、幻覚の傾向を減らすためにトレーニングされました。内部評価では、GPT-4のローンチ版は、オープンドメインの幻覚を避ける点で最新のGPT-3.5モデルよりも19パーセントポイント高く、閉じたドメインの幻覚を避ける点で29パーセントポイント高いスコアを示しました。\n",
      "\n",
      "ただし、この回答は論文の一部の抜粋です。論文全体にはさらなる詳細や結果が含まれています。\n"
     ]
    }
   ],
   "source": [
    "print(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response4 = qa_stuff.run('''\n",
    "論文を日本語で要約してください。ただし、論文の各セクションごとに内容をまとめてください。\n",
    "要約は、以下の形式で、全セクションに対して要約を出力してください。\n",
    "各見出しは、要約に対する適切なタイトルに置き換えてください。\n",
    "\n",
    "## 見出し\n",
    "要約1\n",
    "\n",
    "## 見出し\n",
    "要約2\n",
    "\n",
    "...\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 要約1: 技術報告の概要\n",
      "GPT-4は、画像とテキストの入力を受け取り、テキストの出力を生成する大規模なマルチモーダルモデルです。GPT-4は、次のトークンを予測するために事前学習されたTransformerベースのモデルです。ポストトレーニングのアライメントプロセスにより、事実性や望ましい振る舞いへの遵守度の向上が見られます。このプロジェクトの中心的な要素は、さまざまなスケールで予測可能な振る舞いを示すためのインフラストラクチャと最適化手法の開発でした。\n",
      "\n",
      "## 要約2: 導入\n",
      "この技術報告では、画像とテキストの入出力を処理する大規模なマルチモーダルモデルであるGPT-4を紹介しています。このようなモデルの開発は、対話システム、テキスト要約、機械翻訳など、さまざまなアプリケーションで使用される可能性があるため、近年の関心と進歩の対象となっています。GPT-4は、より複雑で微妙なシナリオでの自然言語テキストの理解と生成能力を向上させることを目指しています。GPT-4は、人間向けに設計されたさまざまな試験での能力をテストしました。これらの評価では、GPT-4は非常に優れた成績を収め、多くの場合、人間の受験者の中でトップ10％に入るスコアを獲得しています。\n",
      "\n",
      "## 要約3: モデルの能力と結果\n",
      "GPT-4は、従来の大規模言語モデルや最先端のシステム（しばしばベンチマーク固有のトレーニングや手作業エンジニアリングを行っている）を上回る性能を発揮します。英語のMMLUベンチマーク[35,36]では、既存のモデルを大幅に上回るだけでなく、他の言語でも強力なパフォーマンスを示します。翻訳されたバリアントのMMLUでは、GPT-4は26の言語のうち24言語で英語の最先端を超えています。モデルの能力と結果については、後のセクションで詳しく説明しています。\n",
      "\n",
      "## 要約4: プロジェクトの課題と結論\n",
      "このプロジェクトの主な課題は、さまざまなスケールで予測可能な振る舞いを示すディープラーニングのインフラストラクチャと最適化手法の開発でした。これにより、同様の方法でトレーニングされた小規模な実行に基づいてGPT-4の予想されるパフォーマンスについて予測することができました。GPT-4は、以前のGPTモデルと同様の制約を持っていますが、その能力にもかかわらず、完全に信頼性があるわけではありません（例：「幻覚」に苦しむことがあります）、限られたコンテキストウィンドウを持ち、学習しないという制約があります。\n"
     ]
    }
   ],
   "source": [
    "print(response4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
